{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\diego.anaya\\OneDrive - Softtek\\Desktop\\SDKTest\\.venv\\lib\\site-packages\\pinecone\\index.py:4: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from softtek_llm.chatbot import Chatbot\n",
    "from softtek_llm.models import OpenAI\n",
    "from softtek_llm.cache import Cache\n",
    "from softtek_llm.vectorStores import PineconeVectorStore\n",
    "from softtek_llm.embeddings import OpenAIEmbeddings\n",
    "from softtek_llm.schemas import Filter\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if OPENAI_API_KEY is None:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in .env file\")\n",
    "\n",
    "OPENAI_API_BASE = os.getenv(\"OPENAI_API_BASE\")\n",
    "if OPENAI_API_BASE is None:\n",
    "    raise ValueError(\"OPENAI_API_BASE not found in .env file\")\n",
    "\n",
    "OPENAI_EMBEDDINGS_MODEL_NAME = os.getenv(\"OPENAI_EMBEDDINGS_MODEL_NAME\")\n",
    "if OPENAI_EMBEDDINGS_MODEL_NAME is None:\n",
    "    raise ValueError(\"OPENAI_EMBEDDINGS_MODEL_NAME not found in .env file\")\n",
    "\n",
    "OPENAI_CHAT_MODEL_NAME = os.getenv(\"OPENAI_CHAT_MODEL_NAME\")\n",
    "if OPENAI_CHAT_MODEL_NAME is None:\n",
    "    raise ValueError(\"OPENAI_CHAT_MODEL_NAME not found in .env file\")\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "if PINECONE_API_KEY is None:\n",
    "    raise ValueError(\"PINECONE_API_KEY not found in .env file\")\n",
    "\n",
    "PINECONE_ENVIRONMENT = os.getenv(\"PINECONE_ENVIRONMENT\")\n",
    "if PINECONE_ENVIRONMENT is None:\n",
    "    raise ValueError(\"PINECONE_ENVIRONMENT not found in .env file\")\n",
    "\n",
    "PINECONE_INDEX_NAME = os.getenv(\"PINECONE_INDEX_NAME\")\n",
    "if PINECONE_INDEX_NAME is None:\n",
    "    raise ValueError(\"PINECONE_INDEX_NAME not found in .env file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = PineconeVectorStore(\n",
    "    api_key=PINECONE_API_KEY,\n",
    "    environment=PINECONE_ENVIRONMENT,\n",
    "    index_name=PINECONE_INDEX_NAME,\n",
    ")\n",
    "embeddings_model = OpenAIEmbeddings(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    model_name=OPENAI_EMBEDDINGS_MODEL_NAME,\n",
    "    api_type=\"azure\",\n",
    "    api_base=OPENAI_API_BASE,\n",
    ")\n",
    "cache = Cache(\n",
    "    vector_store=vector_store,\n",
    "    embeddings_model=embeddings_model,\n",
    ")\n",
    "model = OpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    model_name=OPENAI_CHAT_MODEL_NAME,\n",
    "    api_type=\"azure\",\n",
    "    api_base=OPENAI_API_BASE,\n",
    "    verbose=True,\n",
    ")\n",
    "filters = [\n",
    "    Filter(\n",
    "        type=\"DENY\",\n",
    "        case=\"ANYTHING about the Titanic. YOU CANNOT talk about the Titanic AT ALL.\",\n",
    "    )\n",
    "]\n",
    "chatbot = Chatbot(\n",
    "    model=model,\n",
    "    description=\"You are a very helpful and polite chatbot\",\n",
    "    filters=filters,\n",
    "    cache=cache,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Revision prompt: Considering this context:\n",
      "\n",
      "\n",
      "Please review the prompt below and answer with \"yes\" if it adheres to the rules or \"no\" if it violates any of the rules.\n",
      "Rules:\n",
      "\t- DENY: ANYTHING about the Titanic. YOU CANNOT talk about the Titanic AT ALL.\n",
      "\n",
      "Prompt:\n",
      "Hello! My name is Jeff.\n",
      "Memory: [{'role': 'system', 'content': \"As a prompt reviser, your role is to determine whether the given prompt respects the rules provided below. You can only respond with 'yes' or 'no.' The rules are absolute, meaning that if any of them are not respected, the prompt is considered invalid.\"}, {'role': 'system', 'content': \"only respond with 'yes' or 'no'\"}, {'role': 'user', 'content': 'Considering this context:\\n\\n\\nPlease review the prompt below and answer with \"yes\" if it adheres to the rules or \"no\" if it violates any of the rules.\\nRules:\\n\\t- DENY: ANYTHING about the Titanic. YOU CANNOT talk about the Titanic AT ALL.\\n\\nPrompt:\\nHello! My name is Jeff.'}]\n",
      "Memory: [{'role': 'system', 'content': 'You are a very helpful and polite chatbot'}, {'role': 'user', 'content': 'Hello! My name is Jeff.'}]\n"
     ]
    }
   ],
   "source": [
    "response = chatbot.chat(\n",
    "    \"Hello! My name is Jeff.\",\n",
    "    print_cache_score=True,\n",
    "    cache_kwargs={\"namespace\": \"chatbot-test\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(message=Message(role='assistant', content=\"Hello Jeff! It's nice to meet you. How can I assist you today?\"), created=1695402445, latency=781, from_cache=False, model='gpt-35-turbo-16k', usage=Usage(prompt_tokens=27, completion_tokens=17, total_tokens=44), additional_kwargs={})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Revision prompt: Considering this context:\n",
      "\t- user: Hello! My name is Jeff.\n",
      "\t- assistant: Hello Jeff! It's nice to meet you. How can I assist you today?\n",
      "\n",
      "Please review the prompt below and answer with \"yes\" if it adheres to the rules or \"no\" if it violates any of the rules.\n",
      "Rules:\n",
      "\t- DENY: ANYTHING about the Titanic. YOU CANNOT talk about the Titanic AT ALL.\n",
      "\n",
      "Prompt:\n",
      "What is my name?\n",
      "Memory: [{'role': 'system', 'content': \"As a prompt reviser, your role is to determine whether the given prompt respects the rules provided below. You can only respond with 'yes' or 'no.' The rules are absolute, meaning that if any of them are not respected, the prompt is considered invalid.\"}, {'role': 'system', 'content': \"only respond with 'yes' or 'no'\"}, {'role': 'user', 'content': 'Considering this context:\\n\\t- user: Hello! My name is Jeff.\\n\\t- assistant: Hello Jeff! It\\'s nice to meet you. How can I assist you today?\\n\\nPlease review the prompt below and answer with \"yes\" if it adheres to the rules or \"no\" if it violates any of the rules.\\nRules:\\n\\t- DENY: ANYTHING about the Titanic. YOU CANNOT talk about the Titanic AT ALL.\\n\\nPrompt:\\nWhat is my name?'}]\n",
      "Memory: [{'role': 'system', 'content': 'You are a very helpful and polite chatbot'}, {'role': 'user', 'content': 'Hello! My name is Jeff.'}, {'role': 'assistant', 'content': \"Hello Jeff! It's nice to meet you. How can I assist you today?\"}, {'role': 'user', 'content': 'What is my name?'}]\n"
     ]
    }
   ],
   "source": [
    "response = chatbot.chat(\n",
    "    \"What is my name?\",\n",
    "    print_cache_score=True,\n",
    "    cache_kwargs={\"namespace\": \"chatbot-test\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(message=Message(role='assistant', content='Your name is Jeff, as you mentioned earlier. Is there anything else I can help you with, Jeff?'), created=1695402446, latency=1031, from_cache=False, model='gpt-35-turbo-16k', usage=Usage(prompt_tokens=57, completion_tokens=22, total_tokens=79), additional_kwargs={})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Revision prompt: Considering this context:\n",
      "\t- assistant: Hello Jeff! It's nice to meet you. How can I assist you today?\n",
      "\t- user: What is my name?\n",
      "\t- assistant: Your name is Jeff, as you mentioned earlier. Is there anything else I can help you with, Jeff?\n",
      "\n",
      "Please review the prompt below and answer with \"yes\" if it adheres to the rules or \"no\" if it violates any of the rules.\n",
      "Rules:\n",
      "\t- DENY: ANYTHING about the Titanic. YOU CANNOT talk about the Titanic AT ALL.\n",
      "\n",
      "Prompt:\n",
      "When did the Titanic sink?\n",
      "Memory: [{'role': 'system', 'content': \"As a prompt reviser, your role is to determine whether the given prompt respects the rules provided below. You can only respond with 'yes' or 'no.' The rules are absolute, meaning that if any of them are not respected, the prompt is considered invalid.\"}, {'role': 'system', 'content': \"only respond with 'yes' or 'no'\"}, {'role': 'user', 'content': 'Considering this context:\\n\\t- assistant: Hello Jeff! It\\'s nice to meet you. How can I assist you today?\\n\\t- user: What is my name?\\n\\t- assistant: Your name is Jeff, as you mentioned earlier. Is there anything else I can help you with, Jeff?\\n\\nPlease review the prompt below and answer with \"yes\" if it adheres to the rules or \"no\" if it violates any of the rules.\\nRules:\\n\\t- DENY: ANYTHING about the Titanic. YOU CANNOT talk about the Titanic AT ALL.\\n\\nPrompt:\\nWhen did the Titanic sink?'}]\n"
     ]
    },
    {
     "ename": "InvalidPrompt",
     "evalue": "The prompt does not follow the rules set by the filters. If this behavior is not intended, consider modifying the filters. It is recommended to use LLMs for meta prompts.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidPrompt\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\diego.anaya\\OneDrive - Softtek\\Desktop\\SDKTest\\test.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/diego.anaya/OneDrive%20-%20Softtek/Desktop/SDKTest/test.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m response \u001b[39m=\u001b[39m chatbot\u001b[39m.\u001b[39;49mchat(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/diego.anaya/OneDrive%20-%20Softtek/Desktop/SDKTest/test.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mWhen did the Titanic sink?\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/diego.anaya/OneDrive%20-%20Softtek/Desktop/SDKTest/test.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     print_cache_score\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/diego.anaya/OneDrive%20-%20Softtek/Desktop/SDKTest/test.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     cache_kwargs\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mnamespace\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mchatbot-test\u001b[39;49m\u001b[39m\"\u001b[39;49m},\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/diego.anaya/OneDrive%20-%20Softtek/Desktop/SDKTest/test.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\diego.anaya\\OneDrive - Softtek\\Desktop\\SDKTest\\.venv\\lib\\site-packages\\softtek_llm\\chatbot.py:207\u001b[0m, in \u001b[0;36mChatbot.chat\u001b[1;34m(self, prompt, print_cache_score, cache_kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnon_valid_response:\n\u001b[0;32m    200\u001b[0m             \u001b[39mreturn\u001b[39;00m Response(\n\u001b[0;32m    201\u001b[0m                 message\u001b[39m=\u001b[39mMessage(role\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msystem\u001b[39m\u001b[39m\"\u001b[39m, content\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnon_valid_response),\n\u001b[0;32m    202\u001b[0m                 created\u001b[39m=\u001b[39m\u001b[39mint\u001b[39m(datetime\u001b[39m.\u001b[39mutcnow()\u001b[39m.\u001b[39mtimestamp()),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    205\u001b[0m                 model\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mreviser\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    206\u001b[0m             )\n\u001b[1;32m--> 207\u001b[0m         \u001b[39mraise\u001b[39;00m InvalidPrompt(\n\u001b[0;32m    208\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mThe prompt does not follow the rules set by the filters. If this behavior is not intended, consider modifying the filters. It is recommended to use LLMs for meta prompts.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m         )\n\u001b[0;32m    211\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory\u001b[39m.\u001b[39madd_message(role\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39muser\u001b[39m\u001b[39m\"\u001b[39m, content\u001b[39m=\u001b[39mprompt)\n\u001b[0;32m    212\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache:\n",
      "\u001b[1;31mInvalidPrompt\u001b[0m: The prompt does not follow the rules set by the filters. If this behavior is not intended, consider modifying the filters. It is recommended to use LLMs for meta prompts."
     ]
    }
   ],
   "source": [
    "response = chatbot.chat(\n",
    "    \"When did the Titanic sink?\",\n",
    "    print_cache_score=True,\n",
    "    cache_kwargs={\"namespace\": \"chatbot-test\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot.cache.vector_store.delete(delete_all=True, namespace=\"chatbot-test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
